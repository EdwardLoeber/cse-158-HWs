{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ad6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074d759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_root = 'C:/Users/esloe/OneDrive/Desktop/CSE 158/cse-158-HWs/datasets/'\n",
    "# data = []\n",
    "# f = open(my_root + 'beer_50000.json')\n",
    "# for line in f:\n",
    "#   data.append(eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16198b6e-3975-420c-b325-901e60fa810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataTrain = data[:25000]\n",
    "# dataValid = data[25000:37500]\n",
    "# dataTest = data[37500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67f88ce-1383-4f43-b5cf-2e8022b7556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categoryCounts = defaultdict(int)\n",
    "# for d in data:\n",
    "#     categoryCounts[d['beer/style']] += 1\n",
    "# categories = [c for c in categoryCounts if categoryCounts[c] > 1000]\n",
    "# catID = dict(zip(list(categories),range(len(categories))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81325322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat(d, catID, maxLength, includeCat = True, includeReview = True, includeLength = True):\n",
    "    feat = []\n",
    "    if includeCat:\n",
    "        # My implementation is modular such that this one function concatenates all three features together,\n",
    "        # depending on which are selected\n",
    "        feat.extend([0] * 12)\n",
    "        if d['beer/style'] in (catID) and catID[d['beer/style']]: \n",
    "            feat[catID[d['beer/style']] - 1] = 1\n",
    "    if includeReview:\n",
    "        #\n",
    "        ratings = [d['review/aroma'], d['review/overall'], d['review/appearance'], d['review/palate'], d['review/taste']]\n",
    "        feat.extend(ratings)\n",
    "    if includeLength:\n",
    "        #\n",
    "        len_ratio = len(d['review/text']) / maxLength\n",
    "        feat.append(len_ratio)\n",
    "    return feat + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ada384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(reg, catID, dataTrain, dataValid, dataTest, includeCat=True, includeReview=True, includeLength=True):\n",
    "    mod = linear_model.LogisticRegression(C=reg, class_weight='balanced')\n",
    "\n",
    "    maxLength = max([len(d['review/text']) for d in dataTrain])\n",
    "    \n",
    "    Xtrain = [feat(d, catID, maxLength, includeCat, includeReview, includeLength) for d in dataTrain]\n",
    "    Xvalid = [feat(d, catID, maxLength, includeCat, includeReview, includeLength) for d in dataValid]\n",
    "    Xtest = [feat(d, catID, maxLength, includeCat, includeReview, includeLength) for d in dataTest]\n",
    "    \n",
    "    yTrain = [d['beer/ABV'] > 7 for d in dataTrain]\n",
    "    yValid = [d['beer/ABV'] > 7 for d in dataValid]\n",
    "    yTest = [d['beer/ABV'] > 7 for d in dataTest]\n",
    "\n",
    "    mod.fit(Xtrain,yTrain)\n",
    "\n",
    "    pred = mod.predict(Xvalid)\n",
    "    correct = pred == yValid\n",
    "    TP_ = numpy.logical_and(pred, yValid)\n",
    "    FP_ = numpy.logical_and(pred, numpy.logical_not(yValid))\n",
    "    TN_ = numpy.logical_and(numpy.logical_not(pred), numpy.logical_not(yValid))\n",
    "    FN_ = numpy.logical_and(numpy.logical_not(pred), yValid)\n",
    "\n",
    "    TP = sum(TP_)\n",
    "    FP = sum(FP_)\n",
    "    TN = sum(TN_)\n",
    "    FN = sum(FN_)\n",
    "    \n",
    "    vBER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))\n",
    "\n",
    "    pred = mod.predict(Xtest)\n",
    "    correct = pred == yTest\n",
    "    TP_ = numpy.logical_and(pred, yTest)\n",
    "    FP_ = numpy.logical_and(pred, numpy.logical_not(yTest))\n",
    "    TN_ = numpy.logical_and(numpy.logical_not(pred), numpy.logical_not(yTest))\n",
    "    FN_ = numpy.logical_and(numpy.logical_not(pred), yTest)\n",
    "\n",
    "    TP = sum(TP_)\n",
    "    FP = sum(FP_)\n",
    "    TN = sum(TN_)\n",
    "    FN = sum(FN_)\n",
    "    \n",
    "    tBER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))\n",
    "    \n",
    "    # (1) Fit the model on the training set\n",
    "    # (2) Compute validation BER\n",
    "    # (3) Compute test BER\n",
    "\n",
    "    return mod, vBER, tBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08711296",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f872f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q1(catID, dataTrain, dataValid, dataTest):\n",
    "    # No need to modify this if you've implemented the functions above\n",
    "    mod, validBER, testBER = pipeline(10, catID, dataTrain, dataValid, dataTest, True, False, False)\n",
    "    return mod, validBER, testBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf88be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd1768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q2(catID, dataTrain, dataValid, dataTest):\n",
    "    mod, validBER, testBER = pipeline(10, catID, dataTrain, dataValid, dataTest, True, True, True)\n",
    "    return mod, validBER, testBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eda7330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8985755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q3(catID, dataTrain, dataValid, dataTest):\n",
    "    # Your solution here...\n",
    "    max_BER = 1\n",
    "    for c in [0.001, 0.01, 0.1, 1, 10]:\n",
    "        my_mod, my_validBER, my_testBER = pipeline(c, catID, dataTrain, dataValid, dataTest, True, True, True)\n",
    "        if my_validBER < max_BER:\n",
    "            max_BER = my_validBER\n",
    "            mod, validBER, testBER = my_mod, my_validBER, my_testBER\n",
    "    # Return the validBER and testBER for the model that works best on the validation set\n",
    "    return mod, validBER, testBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af37382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c66382ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q4(C, catID, dataTrain, dataValid, dataTest):\n",
    "    mod, validBER, testBER_noCat = pipeline(C, catID, dataTrain, dataValid, dataTest, False, True, True)\n",
    "    mod, validBER, testBER_noReview = pipeline(C, catID, dataTrain, dataValid, dataTest, True, False, True)\n",
    "    mod, validBER, testBER_noLength = pipeline(C, catID, dataTrain, dataValid, dataTest, True, True, False)\n",
    "    return testBER_noCat, testBER_noReview, testBER_noLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dbd21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d5b6b11-b5d0-4cc5-b724-39f949c4d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_root = 'C:/Users/esloe/OneDrive/Desktop/CSE 158/cse-158-HWs/datasets/'\n",
    "# path = my_root + \"/amazon_reviews_us_Musical_Instruments_v1_00.tsv.gz\"\n",
    "# f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "\n",
    "# header = f.readline()\n",
    "# header = header.strip().split('\\t')\n",
    "# review_dataset = []\n",
    "\n",
    "# pairsSeen = set()\n",
    "\n",
    "# for line in f:\n",
    "#     fields = line.strip().split('\\t')\n",
    "#     d = dict(zip(header, fields))\n",
    "#     ui = (d['customer_id'], d['product_id'])\n",
    "#     if ui in pairsSeen:\n",
    "#         continue\n",
    "#     pairsSeen.add(ui)\n",
    "#     d['star_rating'] = int(d['star_rating'])\n",
    "#     d['helpful_votes'] = int(d['helpful_votes'])\n",
    "#     d['total_votes'] = int(d['total_votes'])\n",
    "#     review_dataset.append(d)\n",
    "# reviewDataTrain = review_dataset[:int(len(review_dataset)*0.9)]\n",
    "# reviewDataTest = review_dataset[int(len(review_dataset)*0.9):]\n",
    "# usersPerItem = defaultdict(set) # Maps an item to the users who rated it\n",
    "# itemsPerUser = defaultdict(set) # Maps a user to the items that they rated\n",
    "# itemNames = {}\n",
    "# ratingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "# reviewsPerUser = defaultdict(list)\n",
    "\n",
    "# for d in reviewDataTrain:\n",
    "#     user,item = d['customer_id'], d['product_id']\n",
    "#     usersPerItem[item].add(user)\n",
    "#     itemsPerUser[user].add(item)\n",
    "#     reviewsPerUser[user].append(d)\n",
    "\n",
    "# for d in review_dataset:\n",
    "#     user,item = d['customer_id'], d['product_id']\n",
    "#     ratingDict[(user,item)] = d['star_rating']\n",
    "#     itemNames[item] = d['product_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "024a628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94773001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar(i, N, usersPerItem):\n",
    "    # Implement...\n",
    "    # Should be a list of (similarity, itemID) pairs\n",
    "    similarities = []\n",
    "    base_users = usersPerItem[i]\n",
    "    for i2 in usersPerItem: #Keys are itemID, dictionary values are users who bought.\n",
    "        if i2 == i:\n",
    "            continue\n",
    "        similarity = Jaccard(base_users,usersPerItem[i2])\n",
    "        \n",
    "        similarities.append((similarity,i2))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d18b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71fc4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf535d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanRating(dataTrain):\n",
    "    # Implement...\n",
    "    mean = sum(d['star_rating'] for d in dataTrain) / len(dataTrain)\n",
    "    return mean\n",
    "\n",
    "def getUserAverages(itemsPerUser, ratingDict):\n",
    "    # Implement (should return a dictionary mapping users to their averages)\n",
    "    userAverages = {}\n",
    "    for user in itemsPerUser:\n",
    "        rating_sum = 0\n",
    "        for i in itemsPerUser[user]:\n",
    "            rating_sum += ratingDict[(user,i)]\n",
    "        userAverages[user] = rating_sum / len(itemsPerUser[user])\n",
    "    return userAverages\n",
    "\n",
    "def getItemAverages(usersPerItem, ratingDict):\n",
    "    # Implement...\n",
    "    itemAverages = {}\n",
    "    for item in usersPerItem:\n",
    "        rating_sum = 0\n",
    "        for u in usersPerItem[item]:\n",
    "            rating_sum += ratingDict[(u,item)]\n",
    "        itemAverages[item] = rating_sum / len(usersPerItem[item])\n",
    "    return itemAverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df9fa7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating(user,item,ratingMean,reviewsPerUser,usersPerItem,itemsPerUser,userAverages,itemAverages):\n",
    "    # Solution for Q6, should return a rating\n",
    "    if item not in itemAverages:\n",
    "        return ratingMean\n",
    "\n",
    "    ratings = []\n",
    "    sims = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        j = d['product_id']\n",
    "        if j == item: continue\n",
    "        ratings.append(d['star_rating'] - itemAverages[j])\n",
    "        sims.append(Jaccard(usersPerItem[item],usersPerItem[j]))\n",
    "\n",
    "    if sum(sims) == 0:\n",
    "        return itemAverages[item]\n",
    "        \n",
    "    if (sum(sims) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,sims)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(sims)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7afad6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "21b64e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRatingQ7(user,item,ratingMean,reviewsPerUser,usersPerItem,itemsPerUser,userAverages,itemAverages):\n",
    "    # Solution for Q7, should return a rating\n",
    "    if item not in itemAverages:\n",
    "        return ratingMean\n",
    "        \n",
    "    if len(usersPerItem.get(item, [])) < 4:\n",
    "        return ratingMean\n",
    "        \n",
    "    ratings = []\n",
    "    sims = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        j = d['product_id']\n",
    "        if j == item: continue\n",
    "        ratings.append(d['star_rating'] - itemAverages[j])\n",
    "        sims.append(Jaccard(usersPerItem[item],usersPerItem[j]))\n",
    "\n",
    "    if sum(sims) == 0:\n",
    "        return itemAverages.get(item, ratingMean)\n",
    "        \n",
    "    if (sum(sims) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,sims)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(sims)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50d28fbb-1051-4875-a456-8f75c62b0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ypred = [predictRatingQ7(d['customer_id'],d['product_id'],ratingMean,reviewsPerUser,usersPerItem,itemsPerUser,userAverages,itemAverages) for d in reviewDataTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2afd6700-4591-4d8f-be81-4da81e477f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratingMean = getMeanRating(reviewDataTrain)\n",
    "# # userAverages = getUserAverages(itemsPerUser, ratingDict)\n",
    "# itemAverages = getItemAverages(usersPerItem, ratingDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf582b1a-a5b2-4630-be1a-3e4667e1c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = [d['star_rating']for d in reviewDataTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20e3f5f5-7310-40cd-bfba-05f8b9be953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE1 = MSE(y,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fb23c05-0a3a-4b29-9552-9f8deeb95c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5879637390482477\n"
     ]
    }
   ],
   "source": [
    "# print(MSE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09f0a4fe-a857-4472-bab6-1cdb7163de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ypred2 = [predictRating(d['customer_id'],d['product_id'],ratingMean,reviewsPerUser,usersPerItem,itemsPerUser,userAverages,itemAverages) for d in reviewDataTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbfcd05f-e775-48ba-a99e-638ed5912341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7165666373340154\n"
     ]
    }
   ],
   "source": [
    "# MSE2 = MSE(y,ypred2)\n",
    "# print(MSE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2dca07fb-89fa-4433-8591-78847de18587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y2 = [ratingMean for d in reviewDataTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b7742ec-e531-4db8-a69f-bfe24afded9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6236571809194924"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE(y2,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a6e46-e0a3-4827-9357-8ed7d6fd68c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
